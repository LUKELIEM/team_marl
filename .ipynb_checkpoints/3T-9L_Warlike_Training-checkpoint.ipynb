{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cultural Parameter Optimization - Warlike\n",
    "\n",
    "### **3T-9L: 3 Teams composed of 9 agents (3 agents per team) **\n",
    "\n",
    "Teams with Warlike culture require much longer training episodes, so we have a separate notebook specifically for  them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.4\n",
      "Pytorch version: 0.4.1.post2\n",
      "OpenAI Gym version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pickle\n",
    "import platform\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# This is the Gathering Game Environment based on Tribal Organization of agents\n",
    "from tribes_env import GatheringEnv\n",
    "from tribes_model import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"OpenAI Gym version: {}\".format(gym.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Culture = Warlike\n",
    "\n",
    "Run finish_episode() specific to this culture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(learners, optimizers, gamma, cuda):\n",
    "    \"\"\" \n",
    "    Note that in RL, policy gradient is calculated at the end of an episode and only then used to \n",
    "    update the weights of an agent's policy. This is very different compared to image recog.\n",
    "    \n",
    "    The code will perform policy update on each learning agent independently. Reward for each time \n",
    "    step is stored in the list policy.rewards[] --> r(t)\n",
    "    \"\"\"  \n",
    "    \n",
    "    num_learners = len(learners)\n",
    "    total_norms = [0 for i in range(num_learners)]\n",
    "    policy_losses = [[] for i in range(num_learners)]\n",
    "    losses = [[] for i in range(num_learners)]\n",
    "    T_reward = []\n",
    "\n",
    "   \n",
    "    for i in range(num_learners):\n",
    "\n",
    "        R = 0\n",
    "        saved_actions = learners[i].saved_actions\n",
    "        \n",
    "        for t in tribes:\n",
    "            if t.name is learners[i].tribe:\n",
    "                T_reward = t.tribal_awards(US_hits = learners[i].US_hits,THEM_hits = learners[i].THEM_hits)\n",
    " \n",
    "                # For debug only\n",
    "                # print('Agent{} receives tribal award from Tribe{}'.format(i,t.name))\n",
    "                # print (T_reward)\n",
    "                # print (learners[i].rewards)\n",
    "                \n",
    "        # Do not implement actor-critic for now\n",
    "        # value_losses = []\n",
    "        \n",
    "        rewards = deque()\n",
    "\n",
    "        for r,T in zip(learners[i].rewards[::-1],T_reward[::-1]):\n",
    "            # The agent is incentivized to cooperate by an award of 30% of what the tribe takes\n",
    "            # in by all its members\n",
    "            R = r + T + gamma * R\n",
    "            rewards.appendleft(R)\n",
    "            \n",
    "        rewards = list(rewards)\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        if cuda:\n",
    "            rewards = rewards.cuda()\n",
    "\n",
    "        # z-score rewards\n",
    "        rewards = (rewards - rewards.mean()) / (1.1e-7+rewards.std())\n",
    "        \n",
    "        #Debug     \n",
    "        #print (rewards)       \n",
    "        \n",
    "        \"\"\"\n",
    "        Do not implement actor-critic for now!!!\n",
    "        for (log_prob, state_value), r in zip(saved_actions, rewards):\n",
    "            reward = r - state_value.data[0]\n",
    "            policy_losses.append(-log_prob * Variable(reward))\n",
    "            r = torch.Tensor([r])\n",
    "            if cuda:\n",
    "                r = r.cuda()\n",
    "            value_losses.append(torch.nn.functional.smooth_l1_loss(state_value,\n",
    "                                                               Variable(r)))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "        loss.backward()        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        for log_prob, r in zip(saved_actions, rewards):\n",
    "            r = torch.Tensor([r])\n",
    "            if cuda:\n",
    "                r = r.cuda()\n",
    "            policy_losses[i].append(-log_prob * Variable(r))\n",
    "\n",
    "        optimizers[i].zero_grad()\n",
    "        losses[i] = torch.stack(policy_losses[i]).sum()\n",
    "        losses[i].backward()\n",
    "        \n",
    "        # Gradient Clipping Update: prevent exploding gradient\n",
    "        total_norms[i] = torch.nn.utils.clip_grad_norm_(learners[i].parameters(), 8000)\n",
    "        \n",
    "        optimizers[i].step()\n",
    "        learners[i].clear_history()   # clear an agent's history at the end of episode\n",
    "\n",
    "\n",
    "    return total_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Case - Warlike Culture\n",
    "\n",
    "We discovered that if we run training of only 300 game steps, the agents with Warlike culture will spend the 1st hundreds of steps in mutually assured destruction - a field of deadly crossfire which they cannot escape out of.\n",
    "\n",
    "So we lengthen the game step to 1000 instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner agent 0\n",
      "Learner agent 1\n",
      "Learner agent 2\n",
      "Learner agent 3\n",
      "Learner agent 4\n",
      "Learner agent 5\n",
      "Learner agent 6\n",
      "Learner agent 7\n",
      "Learner agent 8\n",
      "........................................................................................................................................................................................................\n",
      "Episode 200 complete\n",
      "Learner:0\tReward total:5\tRunning mean: 3.697\n",
      "Learner:1\tReward total:28\tRunning mean: 11.48\n",
      "Learner:2\tReward total:16\tRunning mean: 10.01\n",
      "Learner:3\tReward total:25\tRunning mean: 6.267\n",
      "Learner:4\tReward total:26\tRunning mean: 8.026\n",
      "Learner:5\tReward total:138\tRunning mean: 64.6\n",
      "Learner:6\tReward total:1\tRunning mean: 8.516\n",
      "Learner:7\tReward total:48\tRunning mean: 39.62\n",
      "Learner:8\tReward total:139\tRunning mean: 53.64\n",
      "Max Norms =  ['212.70', '184.58', '260.47', '264.89', '175.34', '222.54', '187.71', '245.93', '226.10']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 400 complete\n",
      "Learner:0\tReward total:23\tRunning mean: 16.14\n",
      "Learner:1\tReward total:42\tRunning mean: 41.62\n",
      "Learner:2\tReward total:122\tRunning mean: 67.15\n",
      "Learner:3\tReward total:25\tRunning mean: 25.97\n",
      "Learner:4\tReward total:15\tRunning mean: 24.48\n",
      "Learner:5\tReward total:31\tRunning mean: 83.13\n",
      "Learner:6\tReward total:14\tRunning mean: 24.11\n",
      "Learner:7\tReward total:46\tRunning mean: 56.64\n",
      "Learner:8\tReward total:59\tRunning mean: 54.07\n",
      "Max Norms =  ['224.74', '496.21', '245.40', '271.68', '154.66', '163.73', '222.40', '208.73', '170.76']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 600 complete\n",
      "Learner:0\tReward total:36\tRunning mean: 24.01\n",
      "Learner:1\tReward total:69\tRunning mean: 57.65\n",
      "Learner:2\tReward total:67\tRunning mean: 62.88\n",
      "Learner:3\tReward total:17\tRunning mean: 26.29\n",
      "Learner:4\tReward total:32\tRunning mean: 21.1\n",
      "Learner:5\tReward total:75\tRunning mean: 71.69\n",
      "Learner:6\tReward total:8\tRunning mean: 21.89\n",
      "Learner:7\tReward total:60\tRunning mean: 51.69\n",
      "Learner:8\tReward total:39\tRunning mean: 43.87\n",
      "Max Norms =  ['212.96', '183.66', '199.23', '179.22', '193.57', '135.98', '216.34', '158.74', '178.14']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 800 complete\n",
      "Learner:0\tReward total:49\tRunning mean: 26.34\n",
      "Learner:1\tReward total:77\tRunning mean: 64.51\n",
      "Learner:2\tReward total:73\tRunning mean: 60.77\n",
      "Learner:3\tReward total:15\tRunning mean: 25.68\n",
      "Learner:4\tReward total:25\tRunning mean: 20.44\n",
      "Learner:5\tReward total:50\tRunning mean: 62.24\n",
      "Learner:6\tReward total:36\tRunning mean: 30.21\n",
      "Learner:7\tReward total:63\tRunning mean: 60.78\n",
      "Learner:8\tReward total:25\tRunning mean: 37.6\n",
      "Max Norms =  ['135.51', '231.49', '236.65', '167.76', '88.08', '104.49', '200.74', '406.90', '123.50']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1000 complete\n",
      "Learner:0\tReward total:43\tRunning mean: 32.63\n",
      "Learner:1\tReward total:67\tRunning mean: 63.83\n",
      "Learner:2\tReward total:65\tRunning mean: 51.86\n",
      "Learner:3\tReward total:23\tRunning mean: 23.48\n",
      "Learner:4\tReward total:11\tRunning mean: 19.79\n",
      "Learner:5\tReward total:60\tRunning mean: 53.73\n",
      "Learner:6\tReward total:59\tRunning mean: 52.82\n",
      "Learner:7\tReward total:45\tRunning mean: 57.87\n",
      "Learner:8\tReward total:11\tRunning mean: 36.54\n",
      "Max Norms =  ['196.21', '196.19', '245.23', '108.89', '103.17', '292.29', '345.01', '257.75', '150.48']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1200 complete\n",
      "Learner:0\tReward total:28\tRunning mean: 35.39\n",
      "Learner:1\tReward total:64\tRunning mean: 63.14\n",
      "Learner:2\tReward total:66\tRunning mean: 49.25\n",
      "Learner:3\tReward total:21\tRunning mean: 21.81\n",
      "Learner:4\tReward total:29\tRunning mean: 18.7\n",
      "Learner:5\tReward total:18\tRunning mean: 51.1\n",
      "Learner:6\tReward total:49\tRunning mean: 62.53\n",
      "Learner:7\tReward total:92\tRunning mean: 55.9\n",
      "Learner:8\tReward total:23\tRunning mean: 28.92\n",
      "Max Norms =  ['182.25', '237.94', '204.17', '131.04', '75.78', '84.11', '214.84', '223.95', '101.67']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1400 complete\n",
      "Learner:0\tReward total:38\tRunning mean: 34.26\n",
      "Learner:1\tReward total:50\tRunning mean: 62.24\n",
      "Learner:2\tReward total:50\tRunning mean: 45.76\n",
      "Learner:3\tReward total:9\tRunning mean: 19.27\n",
      "Learner:4\tReward total:13\tRunning mean: 15.96\n",
      "Learner:5\tReward total:50\tRunning mean: 42.54\n",
      "Learner:6\tReward total:74\tRunning mean: 63.28\n",
      "Learner:7\tReward total:45\tRunning mean: 53.73\n",
      "Learner:8\tReward total:30\tRunning mean: 37.29\n",
      "Max Norms =  ['145.80', '198.96', '232.48', '124.98', '40.38', '166.25', '156.14', '195.62', '186.44']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1600 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 19.99\n",
      "Learner:1\tReward total:0\tRunning mean: 51.61\n",
      "Learner:2\tReward total:0\tRunning mean: 34.85\n",
      "Learner:3\tReward total:0\tRunning mean: 6.14\n",
      "Learner:4\tReward total:0\tRunning mean: 4.356\n",
      "Learner:5\tReward total:0\tRunning mean: 12.71\n",
      "Learner:6\tReward total:0\tRunning mean: 15.6\n",
      "Learner:7\tReward total:0\tRunning mean: 14.0\n",
      "Learner:8\tReward total:0\tRunning mean: 19.82\n",
      "Max Norms =  ['0.00', '0.00', '74.10', '0.00', '0.00', '21.48', '21.97', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1800 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 4.449\n",
      "Learner:1\tReward total:0\tRunning mean: 13.7\n",
      "Learner:2\tReward total:0\tRunning mean: 11.05\n",
      "Learner:3\tReward total:0\tRunning mean: 1.283\n",
      "Learner:4\tReward total:0\tRunning mean: 0.6469\n",
      "Learner:5\tReward total:0\tRunning mean: 1.724\n",
      "Learner:6\tReward total:0\tRunning mean: 2.111\n",
      "Learner:7\tReward total:0\tRunning mean: 1.955\n",
      "Learner:8\tReward total:0\tRunning mean: 3.36\n",
      "Max Norms =  ['0.00', '0.00', '6.46', '0.00', '0.00', '0.48', '8.01', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2000 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.8913\n",
      "Learner:1\tReward total:0\tRunning mean: 2.598\n",
      "Learner:2\tReward total:0\tRunning mean: 2.216\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1719\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1299\n",
      "Learner:5\tReward total:0\tRunning mean: 0.231\n",
      "Learner:6\tReward total:0\tRunning mean: 0.2828\n",
      "Learner:7\tReward total:0\tRunning mean: 0.6515\n",
      "Learner:8\tReward total:0\tRunning mean: 0.4501\n",
      "Max Norms =  ['0.00', '0.00', '0.32', '0.00', '0.00', '0.28', '0.07', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2200 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.9522\n",
      "Learner:1\tReward total:0\tRunning mean: 1.45\n",
      "Learner:2\tReward total:0\tRunning mean: 1.368\n",
      "Learner:3\tReward total:0\tRunning mean: 0.02304\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0174\n",
      "Learner:5\tReward total:0\tRunning mean: 0.03095\n",
      "Learner:6\tReward total:0\tRunning mean: 0.03789\n",
      "Learner:7\tReward total:0\tRunning mean: 0.08728\n",
      "Learner:8\tReward total:0\tRunning mean: 0.06031\n",
      "Max Norms =  ['0.00', '0.00', '33.87', '0.00', '0.00', '0.65', '0.04', '0.00', '0.00']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................\n",
      "Episode 2400 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.1276\n",
      "Learner:1\tReward total:0\tRunning mean: 0.1943\n",
      "Learner:2\tReward total:0\tRunning mean: 0.1832\n",
      "Learner:3\tReward total:0\tRunning mean: 0.003087\n",
      "Learner:4\tReward total:0\tRunning mean: 0.002331\n",
      "Learner:5\tReward total:0\tRunning mean: 0.004146\n",
      "Learner:6\tReward total:0\tRunning mean: 0.005076\n",
      "Learner:7\tReward total:0\tRunning mean: 0.01169\n",
      "Learner:8\tReward total:0\tRunning mean: 0.00808\n",
      "Max Norms =  ['0.00', '0.00', '0.17', '0.00', '0.00', '0.27', '0.03', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2600 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.4404\n",
      "Learner:1\tReward total:0\tRunning mean: 0.3834\n",
      "Learner:2\tReward total:0\tRunning mean: 0.07348\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0004135\n",
      "Learner:4\tReward total:0\tRunning mean: 0.0003123\n",
      "Learner:5\tReward total:0\tRunning mean: 0.0005555\n",
      "Learner:6\tReward total:0\tRunning mean: 0.0006801\n",
      "Learner:7\tReward total:0\tRunning mean: 0.001567\n",
      "Learner:8\tReward total:0\tRunning mean: 0.001083\n",
      "Max Norms =  ['0.00', '0.00', '0.22', '0.00', '0.00', '0.39', '26.35', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2800 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.5357\n",
      "Learner:1\tReward total:0\tRunning mean: 0.6726\n",
      "Learner:2\tReward total:0\tRunning mean: 0.7134\n",
      "Learner:3\tReward total:0\tRunning mean: 5.541e-05\n",
      "Learner:4\tReward total:0\tRunning mean: 4.185e-05\n",
      "Learner:5\tReward total:0\tRunning mean: 7.442e-05\n",
      "Learner:6\tReward total:0\tRunning mean: 9.112e-05\n",
      "Learner:7\tReward total:0\tRunning mean: 0.0002099\n",
      "Learner:8\tReward total:0\tRunning mean: 0.000145\n",
      "Max Norms =  ['0.00', '0.00', '0.08', '0.00', '0.00', '1.43', '13.13', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3000 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.07177\n",
      "Learner:1\tReward total:0\tRunning mean: 0.09011\n",
      "Learner:2\tReward total:0\tRunning mean: 0.09558\n",
      "Learner:3\tReward total:0\tRunning mean: 7.423e-06\n",
      "Learner:4\tReward total:0\tRunning mean: 5.606e-06\n",
      "Learner:5\tReward total:0\tRunning mean: 9.971e-06\n",
      "Learner:6\tReward total:0\tRunning mean: 1.221e-05\n",
      "Learner:7\tReward total:0\tRunning mean: 2.812e-05\n",
      "Learner:8\tReward total:0\tRunning mean: 1.943e-05\n",
      "Max Norms =  ['0.00', '0.00', '0.05', '0.00', '0.00', '0.16', '0.01', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3200 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.04792\n",
      "Learner:1\tReward total:0\tRunning mean: 0.1185\n",
      "Learner:2\tReward total:0\tRunning mean: 0.5831\n",
      "Learner:3\tReward total:0\tRunning mean: 9.946e-07\n",
      "Learner:4\tReward total:0\tRunning mean: 7.511e-07\n",
      "Learner:5\tReward total:0\tRunning mean: 1.336e-06\n",
      "Learner:6\tReward total:0\tRunning mean: 1.636e-06\n",
      "Learner:7\tReward total:0\tRunning mean: 3.768e-06\n",
      "Learner:8\tReward total:0\tRunning mean: 2.603e-06\n",
      "Max Norms =  ['0.00', '0.00', '0.05', '0.00', '0.00', '0.09', '0.02', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3400 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.00642\n",
      "Learner:1\tReward total:0\tRunning mean: 0.01587\n",
      "Learner:2\tReward total:0\tRunning mean: 0.07812\n",
      "Learner:3\tReward total:0\tRunning mean: 1.333e-07\n",
      "Learner:4\tReward total:0\tRunning mean: 1.006e-07\n",
      "Learner:5\tReward total:0\tRunning mean: 1.79e-07\n",
      "Learner:6\tReward total:0\tRunning mean: 2.191e-07\n",
      "Learner:7\tReward total:0\tRunning mean: 5.048e-07\n",
      "Learner:8\tReward total:0\tRunning mean: 3.488e-07\n",
      "Max Norms =  ['0.00', '0.00', '0.04', '0.00', '0.00', '0.09', '0.01', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3600 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0008602\n",
      "Learner:1\tReward total:0\tRunning mean: 0.002127\n",
      "Learner:2\tReward total:0\tRunning mean: 0.01047\n",
      "Learner:3\tReward total:0\tRunning mean: 1.785e-08\n",
      "Learner:4\tReward total:0\tRunning mean: 1.348e-08\n",
      "Learner:5\tReward total:0\tRunning mean: 2.398e-08\n",
      "Learner:6\tReward total:0\tRunning mean: 2.936e-08\n",
      "Learner:7\tReward total:0\tRunning mean: 6.764e-08\n",
      "Learner:8\tReward total:0\tRunning mean: 4.673e-08\n",
      "Max Norms =  ['0.00', '0.00', '0.02', '0.00', '0.00', '0.07', '0.01', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3800 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0001152\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0002849\n",
      "Learner:2\tReward total:0\tRunning mean: 0.001402\n",
      "Learner:3\tReward total:0\tRunning mean: 2.392e-09\n",
      "Learner:4\tReward total:0\tRunning mean: 1.807e-09\n",
      "Learner:5\tReward total:0\tRunning mean: 3.213e-09\n",
      "Learner:6\tReward total:0\tRunning mean: 3.934e-09\n",
      "Learner:7\tReward total:0\tRunning mean: 9.062e-09\n",
      "Learner:8\tReward total:0\tRunning mean: 6.261e-09\n",
      "Max Norms =  ['0.00', '0.00', '0.02', '0.00', '0.00', '0.07', '0.00', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 4000 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.544e-05\n",
      "Learner:1\tReward total:0\tRunning mean: 3.817e-05\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0001879\n",
      "Learner:3\tReward total:0\tRunning mean: 3.205e-10\n",
      "Learner:4\tReward total:0\tRunning mean: 2.42e-10\n",
      "Learner:5\tReward total:0\tRunning mean: 4.305e-10\n",
      "Learner:6\tReward total:0\tRunning mean: 5.27e-10\n",
      "Learner:7\tReward total:0\tRunning mean: 1.214e-09\n",
      "Learner:8\tReward total:0\tRunning mean: 8.389e-10\n",
      "Max Norms =  ['0.00', '0.00', '0.01', '0.00', '0.00', '0.09', '0.00', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 4200 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.1021\n",
      "Learner:1\tReward total:0\tRunning mean: 0.1571\n",
      "Learner:2\tReward total:0\tRunning mean: 0.9271\n",
      "Learner:3\tReward total:0\tRunning mean: 4.294e-11\n",
      "Learner:4\tReward total:0\tRunning mean: 3.243e-11\n",
      "Learner:5\tReward total:0\tRunning mean: 5.767e-11\n",
      "Learner:6\tReward total:0\tRunning mean: 7.061e-11\n",
      "Learner:7\tReward total:0\tRunning mean: 1.627e-10\n",
      "Learner:8\tReward total:0\tRunning mean: 1.124e-10\n",
      "Max Norms =  ['0.00', '0.00', '6.18', '0.00', '0.00', '0.10', '0.00', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 4400 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.01368\n",
      "Learner:1\tReward total:0\tRunning mean: 0.02105\n",
      "Learner:2\tReward total:0\tRunning mean: 0.1242\n",
      "Learner:3\tReward total:0\tRunning mean: 5.753e-12\n",
      "Learner:4\tReward total:0\tRunning mean: 4.345e-12\n",
      "Learner:5\tReward total:0\tRunning mean: 7.727e-12\n",
      "Learner:6\tReward total:0\tRunning mean: 9.46e-12\n",
      "Learner:7\tReward total:0\tRunning mean: 2.179e-11\n",
      "Learner:8\tReward total:0\tRunning mean: 1.506e-11\n",
      "Max Norms =  ['0.00', '0.00', '0.03', '0.00', '0.00', '0.29', '0.00', '0.00', '0.00']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................\n",
      "Episode 4600 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.001833\n",
      "Learner:1\tReward total:0\tRunning mean: 0.002821\n",
      "Learner:2\tReward total:0\tRunning mean: 0.01664\n",
      "Learner:3\tReward total:0\tRunning mean: 7.707e-13\n",
      "Learner:4\tReward total:0\tRunning mean: 5.821e-13\n",
      "Learner:5\tReward total:0\tRunning mean: 1.035e-12\n",
      "Learner:6\tReward total:0\tRunning mean: 1.268e-12\n",
      "Learner:7\tReward total:0\tRunning mean: 2.92e-12\n",
      "Learner:8\tReward total:0\tRunning mean: 2.018e-12\n",
      "Max Norms =  ['0.00', '0.00', '0.03', '0.00', '0.00', '1.82', '0.00', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 4800 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 0.0002456\n",
      "Learner:1\tReward total:0\tRunning mean: 0.0003779\n",
      "Learner:2\tReward total:0\tRunning mean: 0.00223\n",
      "Learner:3\tReward total:0\tRunning mean: 1.033e-13\n",
      "Learner:4\tReward total:0\tRunning mean: 7.799e-14\n",
      "Learner:5\tReward total:0\tRunning mean: 1.387e-13\n",
      "Learner:6\tReward total:0\tRunning mean: 1.698e-13\n",
      "Learner:7\tReward total:0\tRunning mean: 3.912e-13\n",
      "Learner:8\tReward total:0\tRunning mean: 2.703e-13\n",
      "Max Norms =  ['0.00', '0.00', '0.02', '0.00', '0.00', '0.05', '0.00', '0.00', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 5000 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 3.291e-05\n",
      "Learner:1\tReward total:0\tRunning mean: 5.063e-05\n",
      "Learner:2\tReward total:0\tRunning mean: 0.0002987\n",
      "Learner:3\tReward total:0\tRunning mean: 1.384e-14\n",
      "Learner:4\tReward total:0\tRunning mean: 1.045e-14\n",
      "Learner:5\tReward total:0\tRunning mean: 1.858e-14\n",
      "Learner:6\tReward total:0\tRunning mean: 2.275e-14\n",
      "Learner:7\tReward total:0\tRunning mean: 5.242e-14\n",
      "Learner:8\tReward total:0\tRunning mean: 3.622e-14\n",
      "Max Norms =  ['0.00', '0.00', '0.02', '0.00', '0.00', '0.02', '0.00', '0.00', '0.00']\n",
      "Learner agent 0\n",
      "Learner agent 1\n",
      "Learner agent 2\n",
      "Learner agent 3\n",
      "Learner agent 4\n",
      "Learner agent 5\n",
      "Learner agent 6\n",
      "Learner agent 7\n",
      "Learner agent 8\n",
      "........................................................................................................................................................................................................\n",
      "Episode 200 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 1.939\n",
      "Learner:1\tReward total:15\tRunning mean: 5.054\n",
      "Learner:2\tReward total:15\tRunning mean: 8.668\n",
      "Learner:3\tReward total:43\tRunning mean: 11.57\n",
      "Learner:4\tReward total:31\tRunning mean: 8.445\n",
      "Learner:5\tReward total:68\tRunning mean: 22.66\n",
      "Learner:6\tReward total:54\tRunning mean: 26.72\n",
      "Learner:7\tReward total:119\tRunning mean: 46.39\n",
      "Learner:8\tReward total:113\tRunning mean: 94.15\n",
      "Max Norms =  ['181.75', '197.20', '190.75', '231.17', '140.65', '265.39', '166.84', '235.87', '383.34']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 400 complete\n",
      "Learner:0\tReward total:9\tRunning mean: 9.27\n",
      "Learner:1\tReward total:21\tRunning mean: 15.04\n",
      "Learner:2\tReward total:18\tRunning mean: 33.92\n",
      "Learner:3\tReward total:28\tRunning mean: 30.8\n",
      "Learner:4\tReward total:41\tRunning mean: 37.83\n",
      "Learner:5\tReward total:48\tRunning mean: 43.95\n",
      "Learner:6\tReward total:59\tRunning mean: 50.32\n",
      "Learner:7\tReward total:73\tRunning mean: 89.43\n",
      "Learner:8\tReward total:166\tRunning mean: 126.1\n",
      "Max Norms =  ['170.85', '253.57', '203.65', '237.16', '180.08', '227.33', '362.10', '247.28', '261.05']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 600 complete\n",
      "Learner:0\tReward total:16\tRunning mean: 19.91\n",
      "Learner:1\tReward total:37\tRunning mean: 22.03\n",
      "Learner:2\tReward total:37\tRunning mean: 35.33\n",
      "Learner:3\tReward total:26\tRunning mean: 30.34\n",
      "Learner:4\tReward total:22\tRunning mean: 38.94\n",
      "Learner:5\tReward total:63\tRunning mean: 74.5\n",
      "Learner:6\tReward total:95\tRunning mean: 57.26\n",
      "Learner:7\tReward total:35\tRunning mean: 62.15\n",
      "Learner:8\tReward total:92\tRunning mean: 107.0\n",
      "Max Norms =  ['214.38', '231.11', '113.25', '192.80', '101.84', '283.98', '316.30', '139.90', '245.15']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 800 complete\n",
      "Learner:0\tReward total:31\tRunning mean: 26.75\n",
      "Learner:1\tReward total:38\tRunning mean: 25.95\n",
      "Learner:2\tReward total:49\tRunning mean: 48.35\n",
      "Learner:3\tReward total:22\tRunning mean: 29.08\n",
      "Learner:4\tReward total:39\tRunning mean: 39.07\n",
      "Learner:5\tReward total:66\tRunning mean: 73.59\n",
      "Learner:6\tReward total:78\tRunning mean: 80.92\n",
      "Learner:7\tReward total:40\tRunning mean: 42.34\n",
      "Learner:8\tReward total:75\tRunning mean: 87.7\n",
      "Max Norms =  ['128.47', '234.18', '198.13', '267.71', '146.12', '216.64', '232.63', '142.95', '262.84']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1000 complete\n",
      "Learner:0\tReward total:17\tRunning mean: 28.8\n",
      "Learner:1\tReward total:32\tRunning mean: 26.17\n",
      "Learner:2\tReward total:59\tRunning mean: 63.91\n",
      "Learner:3\tReward total:56\tRunning mean: 34.37\n",
      "Learner:4\tReward total:37\tRunning mean: 37.68\n",
      "Learner:5\tReward total:62\tRunning mean: 75.09\n",
      "Learner:6\tReward total:53\tRunning mean: 69.65\n",
      "Learner:7\tReward total:53\tRunning mean: 37.0\n",
      "Learner:8\tReward total:76\tRunning mean: 86.06\n",
      "Max Norms =  ['115.82', '158.22', '165.33', '200.44', '120.06', '220.58', '138.20', '155.20', '226.79']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1200 complete\n",
      "Learner:0\tReward total:22\tRunning mean: 27.53\n",
      "Learner:1\tReward total:19\tRunning mean: 27.14\n",
      "Learner:2\tReward total:58\tRunning mean: 60.04\n",
      "Learner:3\tReward total:55\tRunning mean: 52.86\n",
      "Learner:4\tReward total:50\tRunning mean: 37.02\n",
      "Learner:5\tReward total:65\tRunning mean: 67.85\n",
      "Learner:6\tReward total:66\tRunning mean: 69.0\n",
      "Learner:7\tReward total:67\tRunning mean: 38.44\n",
      "Learner:8\tReward total:59\tRunning mean: 68.54\n",
      "Max Norms =  ['220.68', '210.84', '519.19', '274.41', '132.59', '132.41', '231.66', '136.09', '348.92']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1400 complete\n",
      "Learner:0\tReward total:26\tRunning mean: 23.33\n",
      "Learner:1\tReward total:36\tRunning mean: 22.73\n",
      "Learner:2\tReward total:53\tRunning mean: 51.3\n",
      "Learner:3\tReward total:72\tRunning mean: 60.77\n",
      "Learner:4\tReward total:41\tRunning mean: 34.45\n",
      "Learner:5\tReward total:49\tRunning mean: 69.85\n",
      "Learner:6\tReward total:33\tRunning mean: 59.4\n",
      "Learner:7\tReward total:23\tRunning mean: 38.9\n",
      "Learner:8\tReward total:95\tRunning mean: 70.61\n",
      "Max Norms =  ['80.79', '174.39', '168.19', '237.23', '166.76', '361.65', '172.49', '79.84', '285.01']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1600 complete\n",
      "Learner:0\tReward total:20\tRunning mean: 28.33\n",
      "Learner:1\tReward total:30\tRunning mean: 26.37\n",
      "Learner:2\tReward total:75\tRunning mean: 54.54\n",
      "Learner:3\tReward total:62\tRunning mean: 63.27\n",
      "Learner:4\tReward total:41\tRunning mean: 31.65\n",
      "Learner:5\tReward total:24\tRunning mean: 53.4\n",
      "Learner:6\tReward total:66\tRunning mean: 57.21\n",
      "Learner:7\tReward total:59\tRunning mean: 37.93\n",
      "Learner:8\tReward total:113\tRunning mean: 88.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['189.90', '176.47', '182.85', '322.73', '66.99', '192.45', '217.39', '141.59', '266.06']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1800 complete\n",
      "Learner:0\tReward total:30\tRunning mean: 30.14\n",
      "Learner:1\tReward total:25\tRunning mean: 27.68\n",
      "Learner:2\tReward total:54\tRunning mean: 52.68\n",
      "Learner:3\tReward total:55\tRunning mean: 68.17\n",
      "Learner:4\tReward total:20\tRunning mean: 27.59\n",
      "Learner:5\tReward total:59\tRunning mean: 49.67\n",
      "Learner:6\tReward total:65\tRunning mean: 59.86\n",
      "Learner:7\tReward total:22\tRunning mean: 29.74\n",
      "Learner:8\tReward total:71\tRunning mean: 85.36\n",
      "Max Norms =  ['128.24', '142.98', '311.65', '252.46', '137.79', '155.83', '193.04', '58.31', '207.15']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2000 complete\n",
      "Learner:0\tReward total:29\tRunning mean: 30.17\n",
      "Learner:1\tReward total:40\tRunning mean: 25.78\n",
      "Learner:2\tReward total:46\tRunning mean: 56.25\n",
      "Learner:3\tReward total:82\tRunning mean: 77.44\n",
      "Learner:4\tReward total:11\tRunning mean: 26.01\n",
      "Learner:5\tReward total:41\tRunning mean: 48.41\n",
      "Learner:6\tReward total:85\tRunning mean: 48.53\n",
      "Learner:7\tReward total:21\tRunning mean: 30.07\n",
      "Learner:8\tReward total:63\tRunning mean: 75.45\n",
      "Max Norms =  ['190.28', '142.65', '330.44', '221.91', '84.47', '212.17', '228.71', '67.67', '232.88']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2200 complete\n",
      "Learner:0\tReward total:35\tRunning mean: 29.88\n",
      "Learner:1\tReward total:15\tRunning mean: 22.87\n",
      "Learner:2\tReward total:69\tRunning mean: 58.53\n",
      "Learner:3\tReward total:107\tRunning mean: 81.67\n",
      "Learner:4\tReward total:20\tRunning mean: 23.8\n",
      "Learner:5\tReward total:52\tRunning mean: 46.36\n",
      "Learner:6\tReward total:43\tRunning mean: 50.42\n",
      "Learner:7\tReward total:27\tRunning mean: 30.73\n",
      "Learner:8\tReward total:56\tRunning mean: 72.38\n",
      "Max Norms =  ['304.42', '249.74', '168.18', '298.00', '25.76', '223.97', '189.58', '73.34', '274.07']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2400 complete\n",
      "Learner:0\tReward total:16\tRunning mean: 27.69\n",
      "Learner:1\tReward total:27\tRunning mean: 22.43\n",
      "Learner:2\tReward total:52\tRunning mean: 56.2\n",
      "Learner:3\tReward total:72\tRunning mean: 82.89\n",
      "Learner:4\tReward total:32\tRunning mean: 25.21\n",
      "Learner:5\tReward total:45\tRunning mean: 42.3\n",
      "Learner:6\tReward total:89\tRunning mean: 51.21\n",
      "Learner:7\tReward total:35\tRunning mean: 31.28\n",
      "Learner:8\tReward total:97\tRunning mean: 78.04\n",
      "Max Norms =  ['232.58', '65.57', '193.44', '265.91', '96.87', '219.42', '228.10', '170.00', '236.35']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2600 complete\n",
      "Learner:0\tReward total:22\tRunning mean: 27.32\n",
      "Learner:1\tReward total:11\tRunning mean: 19.61\n",
      "Learner:2\tReward total:45\tRunning mean: 47.45\n",
      "Learner:3\tReward total:133\tRunning mean: 93.89\n",
      "Learner:4\tReward total:28\tRunning mean: 21.47\n",
      "Learner:5\tReward total:39\tRunning mean: 43.48\n",
      "Learner:6\tReward total:35\tRunning mean: 50.41\n",
      "Learner:7\tReward total:26\tRunning mean: 24.32\n",
      "Learner:8\tReward total:73\tRunning mean: 74.71\n",
      "Max Norms =  ['172.13', '181.36', '122.18', '348.39', '184.70', '173.33', '257.71', '57.56', '365.14']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2800 complete\n",
      "Learner:0\tReward total:21\tRunning mean: 30.37\n",
      "Learner:1\tReward total:24\tRunning mean: 20.74\n",
      "Learner:2\tReward total:48\tRunning mean: 47.78\n",
      "Learner:3\tReward total:85\tRunning mean: 95.9\n",
      "Learner:4\tReward total:28\tRunning mean: 22.97\n",
      "Learner:5\tReward total:96\tRunning mean: 42.2\n",
      "Learner:6\tReward total:49\tRunning mean: 50.34\n",
      "Learner:7\tReward total:26\tRunning mean: 26.45\n",
      "Learner:8\tReward total:40\tRunning mean: 72.53\n",
      "Max Norms =  ['146.18', '221.60', '263.52', '299.79', '99.24', '275.75', '156.30', '131.36', '283.57']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3000 complete\n",
      "Learner:0\tReward total:32\tRunning mean: 27.96\n",
      "Learner:1\tReward total:17\tRunning mean: 23.65\n",
      "Learner:2\tReward total:28\tRunning mean: 48.26\n",
      "Learner:3\tReward total:111\tRunning mean: 102.9\n",
      "Learner:4\tReward total:17\tRunning mean: 21.15\n",
      "Learner:5\tReward total:63\tRunning mean: 43.45\n",
      "Learner:6\tReward total:17\tRunning mean: 43.74\n",
      "Learner:7\tReward total:43\tRunning mean: 27.98\n",
      "Learner:8\tReward total:84\tRunning mean: 65.55\n",
      "Max Norms =  ['177.73', '240.03', '131.53', '284.86', '23.36', '203.42', '211.96', '197.04', '233.89']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3200 complete\n",
      "Learner:0\tReward total:30\tRunning mean: 28.45\n",
      "Learner:1\tReward total:26\tRunning mean: 26.22\n",
      "Learner:2\tReward total:51\tRunning mean: 55.42\n",
      "Learner:3\tReward total:88\tRunning mean: 100.6\n",
      "Learner:4\tReward total:19\tRunning mean: 20.59\n",
      "Learner:5\tReward total:47\tRunning mean: 46.1\n",
      "Learner:6\tReward total:60\tRunning mean: 44.13\n",
      "Learner:7\tReward total:17\tRunning mean: 30.92\n",
      "Learner:8\tReward total:50\tRunning mean: 75.13\n",
      "Max Norms =  ['258.08', '144.65', '194.76', '242.83', '25.85', '287.01', '273.25', '147.45', '254.57']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3400 complete\n",
      "Learner:0\tReward total:32\tRunning mean: 31.06\n",
      "Learner:1\tReward total:18\tRunning mean: 18.46\n",
      "Learner:2\tReward total:73\tRunning mean: 48.88\n",
      "Learner:3\tReward total:84\tRunning mean: 89.07\n",
      "Learner:4\tReward total:22\tRunning mean: 22.66\n",
      "Learner:5\tReward total:55\tRunning mean: 48.98\n",
      "Learner:6\tReward total:53\tRunning mean: 47.59\n",
      "Learner:7\tReward total:56\tRunning mean: 36.4\n",
      "Learner:8\tReward total:20\tRunning mean: 80.31\n",
      "Max Norms =  ['183.23', '90.97', '153.18', '276.47', '106.07', '187.79', '302.62', '180.90', '180.46']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3600 complete\n",
      "Learner:0\tReward total:24\tRunning mean: 31.37\n",
      "Learner:1\tReward total:27\tRunning mean: 18.75\n",
      "Learner:2\tReward total:59\tRunning mean: 55.86\n",
      "Learner:3\tReward total:73\tRunning mean: 82.22\n",
      "Learner:4\tReward total:9\tRunning mean: 18.85\n",
      "Learner:5\tReward total:46\tRunning mean: 43.76\n",
      "Learner:6\tReward total:37\tRunning mean: 50.48\n",
      "Learner:7\tReward total:63\tRunning mean: 36.17\n",
      "Learner:8\tReward total:71\tRunning mean: 85.76\n",
      "Max Norms =  ['147.06', '157.98', '195.68', '255.36', '33.19', '151.48', '202.65', '186.14', '196.01']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3800 complete\n",
      "Learner:0\tReward total:46\tRunning mean: 29.01\n",
      "Learner:1\tReward total:9\tRunning mean: 15.37\n",
      "Learner:2\tReward total:30\tRunning mean: 48.28\n",
      "Learner:3\tReward total:94\tRunning mean: 81.08\n",
      "Learner:4\tReward total:11\tRunning mean: 23.98\n",
      "Learner:5\tReward total:48\tRunning mean: 47.24\n",
      "Learner:6\tReward total:48\tRunning mean: 46.24\n",
      "Learner:7\tReward total:3\tRunning mean: 30.35\n",
      "Learner:8\tReward total:102\tRunning mean: 85.85\n",
      "Max Norms =  ['192.16', '183.61', '273.46', '208.76', '37.51', '162.81', '202.71', '51.54', '326.13']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................\n",
      "Episode 4000 complete\n",
      "Learner:0\tReward total:28\tRunning mean: 34.68\n",
      "Learner:1\tReward total:21\tRunning mean: 14.81\n",
      "Learner:2\tReward total:33\tRunning mean: 52.86\n",
      "Learner:3\tReward total:104\tRunning mean: 70.27\n",
      "Learner:4\tReward total:9\tRunning mean: 18.44\n",
      "Learner:5\tReward total:44\tRunning mean: 38.2\n",
      "Learner:6\tReward total:44\tRunning mean: 53.2\n",
      "Learner:7\tReward total:17\tRunning mean: 31.72\n",
      "Learner:8\tReward total:60\tRunning mean: 94.57\n",
      "Max Norms =  ['89.48', '86.74', '288.18', '303.50', '35.04', '136.84', '284.20', '68.63', '264.93']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 4200 complete\n",
      "Learner:0\tReward total:23\tRunning mean: 28.5\n",
      "Learner:1\tReward total:11\tRunning mean: 18.63\n",
      "Learner:2\tReward total:73\tRunning mean: 53.86\n",
      "Learner:3\tReward total:93\tRunning mean: 96.1\n",
      "Learner:4\tReward total:29\tRunning mean: 18.84\n",
      "Learner:5\tReward total:56\tRunning mean: 41.16\n",
      "Learner:6\tReward total:28\tRunning mean: 42.89\n",
      "Learner:7\tReward total:44\tRunning mean: 25.66\n",
      "Learner:8\tReward total:34\tRunning mean: 65.37\n",
      "Max Norms =  ['169.90', '145.13', '323.33', '382.82', '91.71', '347.14', '400.91', '159.65', '164.21']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 4400 complete\n",
      "Learner:0\tReward total:49\tRunning mean: 30.61\n",
      "Learner:1\tReward total:21\tRunning mean: 16.64\n",
      "Learner:2\tReward total:41\tRunning mean: 53.8\n",
      "Learner:3\tReward total:68\tRunning mean: 85.9\n",
      "Learner:4\tReward total:12\tRunning mean: 19.68\n",
      "Learner:5\tReward total:18\tRunning mean: 41.71\n",
      "Learner:6\tReward total:24\tRunning mean: 50.99\n",
      "Learner:7\tReward total:11\tRunning mean: 28.59\n",
      "Learner:8\tReward total:101\tRunning mean: 59.52\n",
      "Max Norms =  ['148.82', '188.13', '287.19', '168.59', '18.39', '384.07', '165.77', '44.45', '295.43']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 4600 complete\n",
      "Learner:0\tReward total:47\tRunning mean: 29.35\n",
      "Learner:1\tReward total:19\tRunning mean: 18.58\n",
      "Learner:2\tReward total:49\tRunning mean: 52.29\n",
      "Learner:3\tReward total:51\tRunning mean: 71.16\n",
      "Learner:4\tReward total:25\tRunning mean: 22.46\n",
      "Learner:5\tReward total:74\tRunning mean: 48.61\n",
      "Learner:6\tReward total:77\tRunning mean: 51.67\n",
      "Learner:7\tReward total:38\tRunning mean: 33.12\n",
      "Learner:8\tReward total:52\tRunning mean: 67.85\n",
      "Max Norms =  ['176.11', '332.10', '258.25', '285.67', '30.61', '209.73', '256.71', '69.06', '186.12']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 4800 complete\n",
      "Learner:0\tReward total:19\tRunning mean: 27.4\n",
      "Learner:1\tReward total:17\tRunning mean: 20.01\n",
      "Learner:2\tReward total:23\tRunning mean: 54.12\n",
      "Learner:3\tReward total:94\tRunning mean: 68.18\n",
      "Learner:4\tReward total:23\tRunning mean: 21.8\n",
      "Learner:5\tReward total:39\tRunning mean: 43.32\n",
      "Learner:6\tReward total:43\tRunning mean: 59.49\n",
      "Learner:7\tReward total:26\tRunning mean: 32.19\n",
      "Learner:8\tReward total:56\tRunning mean: 57.32\n",
      "Max Norms =  ['140.35', '186.39', '207.12', '377.65', '60.65', '78.95', '216.75', '244.20', '131.87']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 5000 complete\n",
      "Learner:0\tReward total:21\tRunning mean: 22.86\n",
      "Learner:1\tReward total:23\tRunning mean: 19.6\n",
      "Learner:2\tReward total:62\tRunning mean: 48.53\n",
      "Learner:3\tReward total:35\tRunning mean: 73.87\n",
      "Learner:4\tReward total:35\tRunning mean: 22.78\n",
      "Learner:5\tReward total:98\tRunning mean: 53.37\n",
      "Learner:6\tReward total:51\tRunning mean: 75.02\n",
      "Learner:7\tReward total:42\tRunning mean: 25.41\n",
      "Learner:8\tReward total:92\tRunning mean: 60.35\n",
      "Max Norms =  ['110.30', '316.94', '261.24', '274.26', '152.06', '257.20', '167.14', '95.52', '202.92']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import sys\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "# Initialize agents parameters\n",
    "#   9 agents - 9 learning agents, 0 trained agent, 0 random agent\n",
    "num_learners = 9\n",
    "num_trained = 0\n",
    "num_rdn = 0\n",
    "num_statics = num_trained + num_rdn\n",
    "num_agents = num_learners + num_statics  \n",
    "\n",
    "# Initialize environment\n",
    "game = \"Gather\"\n",
    "num_actions = 8                       # 8 actions in Gathering\n",
    "\n",
    "# Initialize training parameters\n",
    "warm_start = False\n",
    "num_frames = 4      # environ observation consists of a list of 4 stacked frames per agent\n",
    "max_episodes = 5000\n",
    "max_frames = 1000\n",
    "max_frames_ep = 0   # track highest number of frames an episode can last\n",
    "\n",
    "# These trainer parameters works for Atari Breakout\n",
    "gamma = 0.99  \n",
    "lr = 1e-3\n",
    "temp_start = 1.8  # Temperature for explore/exploit\n",
    "temp_end = 1.0\n",
    "log_interval = 200\n",
    "save_interval = 500\n",
    "\n",
    "\n",
    "def unpack_env_obs(env_obs):\n",
    "    \"\"\"\n",
    "    Gathering is a partially-observable Markov Game. env_obs returned by GatheringEnv is a numpy \n",
    "    array of dimension (num_agent, 800), which represents the agents' observations of the game.\n",
    "\n",
    "    The 800 elements (view_box) encodes 4 layers of 10x20 pixels frames in the format:\n",
    "    (viewbox_width, viewbox_depth, 4).\n",
    "    \n",
    "    This code reshapes the above into stacked frames that can be accepted by the Policy class:\n",
    "    (batch_idx, in_channel, width, height)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    num_agents = len(env_obs)  # environ observations is a list of agents' observations\n",
    "    \n",
    "    obs = []\n",
    "    for i in range(num_agents):\n",
    "        x = env_obs[i]   # take the indexed agent's observation\n",
    "        x = torch.Tensor(x)   # Convert to tensor\n",
    "        \n",
    "        # Policy is a 3-layer CNN\n",
    "        x = x.view(1, 10, 20, -1)  # reshape into environment defined stacked frames\n",
    "        x = x.permute(0, 3, 1, 2)  # permute to Policy accepted stacked frames\n",
    "        obs.append(x)\n",
    "        \n",
    "    return obs  # return a list of Tensors\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For now, we do not implement LSTM            \n",
    "# LSTM Change: Need to cycle hx and cx thru function\n",
    "def select_action(model, state, lstm_hc, cuda):\n",
    "    hx , cx = lstm_hc \n",
    "    num_frames, height, width = state.shape\n",
    "    state = torch.FloatTensor(state.reshape(-1, num_frames, height, width))\n",
    "\n",
    "    if cuda:\n",
    "        state = state.cuda()\n",
    "\n",
    "    probs, value, (hx, cx) = model((Variable(state), (hx, cx)))\n",
    "\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    # LSTM Change: Need to cycle hx and cx thru function\n",
    "    return action.data[0], log_prob, value, (hx, cx)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def select_learner_action(model, obs, cuda):\n",
    "    \"\"\"\n",
    "    This code expects obs to be an array of stacked frames of the following dim:\n",
    "    (batch_idx, in_channel, width, height)\n",
    "    \n",
    "    This is inputted into model - the agent's Policy, which outputs a probability \n",
    "    distribution over available actions.\n",
    "    \n",
    "    Policy gradient is implemented using torch.distributions.Categorical. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Policy is a 3-layer CNN\n",
    "    # _, num_frames, width, height = obs.shape\n",
    "    # obs = torch.FloatTensor(obs.reshape(-1, num_frames, width, height))\n",
    "    \n",
    "    # Policy is a 2-layer NN for now\n",
    "    # obs = obs.view(1, -1)\n",
    "   \n",
    "    if cuda:\n",
    "        obs = obs.cuda()\n",
    "      \n",
    "    probs = model(obs)\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "\n",
    "    return action.item(), log_prob \n",
    "\n",
    "\n",
    "def load_info(agents, narrate=False):\n",
    "    for i in range(num_agents):    \n",
    "        agents[i].load_info(info[i])\n",
    "        if narrate:\n",
    "            if agents[i].tagged:\n",
    "                print('frame {}, agent{} is tagged'.format(frame,i))\n",
    "            if agents[i].laser_fired:\n",
    "                print('frame {}, agent{} fires its laser'.format(frame,i))\n",
    "                print('and hit {} US and {} THEM'.format(agents[i].US_hit, agents[i].THEM_hit))\n",
    "    return\n",
    "\n",
    "\n",
    "# The main code starts here!!!\n",
    "\n",
    "cultures =[{'name':'warlike', 'penalty':-1.0, 'reward':0.075},\n",
    "           {'name':'warlike', 'penalty':-1.0, 'reward':0.025}\n",
    "          ]\n",
    "\n",
    "\"\"\"\n",
    "cultures =[\n",
    "           {'name':'warlike', 'penalty':-1.0, 'reward':0.001},\n",
    "           {'name':'warlike', 'penalty':-1.0, 'reward':0.005},\n",
    "           {'name':'warlike', 'penalty':-1.0, 'reward':0.01},\n",
    "           {'name':'warlike', 'penalty':-1.0, 'reward':0.05},\n",
    "           {'name':'warlike', 'penalty':-1.0, 'reward':0.1},\n",
    "           {'name':'warlike', 'penalty':-1.0, 'reward':0.5},\n",
    "           {'name':'warlike', 'penalty':-1.0, 'reward':1.0}\n",
    "          ]\n",
    "\"\"\"\n",
    "\n",
    "# Cultural parameter search\n",
    "for culture in cultures:   # Go down the list of cultures\n",
    "\n",
    "    # Data structure for agents\n",
    "    agents = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    tags = []\n",
    "    rewards = []\n",
    "    optimizers = []\n",
    "\n",
    "    # Cold start\n",
    "    if warm_start is False:\n",
    "   \n",
    "        # Initialize learner agents, then load static agents (trained followed by random)\n",
    "        for i in range(num_learners):\n",
    "            print(\"Learner agent {}\".format(i))\n",
    "            agents.append(Policy(num_frames, num_actions, i)) # No weights loaded for learning agent\n",
    "            optimizers.append(optim.Adam(agents[i].parameters(), lr=lr))\n",
    "        \n",
    "            # set up optimizer - this works for Atari Breakout\n",
    "            # optimizers.append(optim.RMSprop(agents[i].parameters(), lr=lr, weight_decay=0.1)) \n",
    "        \n",
    "        for i in range(num_learners, num_learners+num_trained):\n",
    "            print (\"No trained agent exist yet!\")\n",
    "            raise\n",
    "            \"\"\"\n",
    "            Disable for now! No trained model exist!!!\n",
    "            agents.append(Policy(num_frames, num_actions, i))\n",
    "            agents[i].load_weights()         # load weight for static agent        \n",
    "            \"\"\"\n",
    "        for i in range(num_learners+num_trained, num_agents):\n",
    "            print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "\n",
    "    \n",
    "        # Initialize all agent data\n",
    "        actions = [0 for i in range(num_agents)]\n",
    "        log_probs = [0 for i in range(num_agents)]\n",
    "        tags = [0 for i in range(num_agents)]\n",
    "        rewards = [0 for i in range(num_agents)]\n",
    "\n",
    "        # Keep track of rewards learned by learners\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        running_reward = [None for i in range(num_learners)]   # running average\n",
    "        running_rewards = [[] for i in range(num_learners)]   # history of running averages\n",
    "        best_reward = [0 for i in range(num_learners)]    # best running average (for storing best_model)\n",
    "\n",
    "        # This is to support warm start for training\n",
    "        prior_eps = 0\n",
    "\n",
    "    # Warm start\n",
    "    if warm_start:\n",
    "        print (\"Cannot warm start\")\n",
    "        raise\n",
    "    \n",
    "        \"\"\"\n",
    "        # Disable for now!  Need to ensure model can support training on GPU and game playing\n",
    "        # on both CPU and GPU.\n",
    "    \n",
    "        data_file = 'results/{}.p'.format(game)\n",
    "\n",
    "        try:\n",
    "            with open(data_file, 'rb') as f:\n",
    "                running_rewards = pickle.load(f)\n",
    "                running_reward = running_rewards[-1]\n",
    "\n",
    "            prior_eps = len(running_rewards)\n",
    "\n",
    "            model_file = 'saved_models/actor_critic_{}_ep_{}.p'.format(game, prior_eps)\n",
    "            with open(model_file, 'rb') as f:\n",
    "                # Model Save and Load Update: Include both model and optim parameters\n",
    "                saved_model = pickle.load(f)\n",
    "                model, optimizer = saved_model\n",
    "\n",
    "        except OSError:\n",
    "            print('Saved file not found. Creating new cold start model.')\n",
    "            model = Policy(input_channels=num_frames, num_actions=num_actions)\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=lr,\n",
    "                                      weight_decay=0.1)\n",
    "            running_rewards = []\n",
    "            prior_eps = 0\n",
    "        \"\"\"\n",
    "\n",
    "    # Establish tribal association\n",
    "\n",
    "    tribes = []\n",
    "    tribes.append(Tribe(name='Vikings',color='blue', culture=culture, \\\n",
    "                    agents=[agents[0], agents[1], agents[2]]))\n",
    "    tribes.append(Tribe(name='Saxons', color='red', culture=culture, \\\n",
    "                    agents=[agents[3], agents[4], agents[5]]))\n",
    "    tribes.append(Tribe(name='Franks', color='purple', culture=culture, \\\n",
    "                    agents=[agents[6], agents[7], agents[8]]))\n",
    "    # tribes.append(Tribe(name='Crazies', color='yellow', agents=[agents[9]]))   # random agents are crazy!!!\n",
    "\n",
    "    # 9 agents in 4 tribes, used map defined in default.txt\n",
    "    agent_colors = [agent.color for agent in agents]\n",
    "    agent_tribes = [agent.tribe for agent in agents]\n",
    "    \n",
    "    env = GatheringEnv(n_agents=num_agents,agent_colors=agent_colors, agent_tribes=agent_tribes, \\\n",
    "                       map_name='default')    \n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    if cuda:\n",
    "        for i in range(num_learners):    # Learning agents need to utilize GPU\n",
    "            agents[i].cuda()\n",
    "\n",
    "        \n",
    "    for ep in range(max_episodes):\n",
    "    \n",
    "        print('.', end='')  # To show progress\n",
    "    \n",
    "        # Anneal temperature from temp_start to temp_end\n",
    "        for i in range(num_learners):    # For learning agents\n",
    "            agents[i].temperature = max(temp_end, temp_start - (temp_start - temp_end) * (ep / max_episodes))\n",
    "\n",
    "        env_obs = env.reset()  # Env return observations\n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(env_obs))\n",
    "        # print (env_obs[0].shape)\n",
    "    \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "\n",
    "        for i in range(num_learners):    # Reset agent info - laser tag statistics\n",
    "            agents[i].reset_info()   \n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(agents_obs))\n",
    "        # print (agents_obs[0].shape)\n",
    "    \n",
    "        \"\"\"\n",
    "        For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "        state = np.stack([state]*num_frames)\n",
    "\n",
    "        # LSTM change - reset LSTM hidden units when episode begins\n",
    "        cx = Variable(torch.zeros(1, 256))\n",
    "        hx = Variable(torch.zeros(1, 256))\n",
    "        if cuda:\n",
    "            cx = cx.cuda()\n",
    "            hx = hx.cuda()\n",
    "        \"\"\"\n",
    "\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "    \n",
    "        for frame in range(max_frames):\n",
    "\n",
    "            \"\"\"\n",
    "            For now, we do not implement LSTM\n",
    "            # Select action\n",
    "            # LSTM Change: Need to cycle hx and cx thru select_action\n",
    "            action, log_prob, value, (hx,cx)  = select_action(model, state, (hx,cx), cuda)        \n",
    "            \"\"\"\n",
    "\n",
    "            for i in range(num_learners):    # For learning agents\n",
    "                actions[i], log_probs[i] = select_learner_action(agents[i], agents_obs[i], cuda)\n",
    "                if actions[i] is 6:\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                agents[i].saved_actions.append((log_probs[i]))\n",
    "            \n",
    "                # Do not implement LSTM for now\n",
    "                # actions[i].saved_actions.append((log_prob, value))\n",
    "            \n",
    "            for i in range(num_learners, num_learners+num_trained):\n",
    "                print (\"No trained agent exist yet!\")\n",
    "                raise\n",
    "            for i in range(num_learners+num_trained, num_agents):   # For random agents\n",
    "                actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                if actions[i] is 6:\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "\n",
    "            # For Debug only\n",
    "            # if frame % 20 == 0:\n",
    "            #    print (actions) \n",
    "            #    print (log_probs)\n",
    "            \n",
    "            # Perform step        \n",
    "            env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "            \"\"\"\n",
    "            For Debug only\n",
    "            print (env_obs)\n",
    "            print (reward)\n",
    "            print (done) \n",
    "            \"\"\"\n",
    "       \n",
    "            # Unpack observations into data structure compatible with agent Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "            load_info(agents, narrate=False)   # Load agent info for AI agents\n",
    "\n",
    "            # For learner agents only, generate reward statistics and reward stack for policy gradient\n",
    "            for i in range(num_learners):\n",
    "                agents[i].rewards.append(reward[i])  # Stack rewards (for policy gradient)\n",
    "                episode_reward[i] += reward[i]   # accumulate episode reward \n",
    "            \n",
    "            \"\"\"\n",
    "            For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "            # Evict oldest diff add new diff to state\n",
    "            next_state = np.stack([next_state]*num_frames)\n",
    "            next_state[1:, :, :] = state[:-1, :, :]\n",
    "            state = next_state\n",
    "            \"\"\"\n",
    "\n",
    "            if any(done):\n",
    "                print(\"Done after {} frames\".format(frame))\n",
    "                break\n",
    "            \n",
    "        if frame > max_frames_ep:\n",
    "            max_frames_ep = frame    # Keep track of highest frames/episode\n",
    "\n",
    "        # Update reward statistics for learners\n",
    "        for i in range(num_learners):\n",
    "            if running_reward[i] is None:\n",
    "                running_reward[i] = episode_reward[i]\n",
    "            running_reward[i] = running_reward[i] * 0.99 + episode_reward[i] * 0.01\n",
    "            running_rewards[i].append(running_reward[i])\n",
    "\n",
    "        # Track Episode #, temp and highest frames/episode\n",
    "        if (ep+prior_eps+1) % log_interval == 0: \n",
    "            verbose_str = '\\nEpisode {} complete'.format(ep+prior_eps+1)\n",
    "            # verbose_str += '\\tTemp = {:.4}'.format(model.temperature)\n",
    "            # verbose_str += '\\tMax frames = {}'.format(max_frames_ep+1)\n",
    "            print(verbose_str)\n",
    "    \n",
    "            # Display rewards and running rewards for learning agents\n",
    "            for i in range(num_learners):\n",
    "                verbose_str = 'Learner:{}'.format(i)\n",
    "                verbose_str += '\\tReward total:{}'.format(episode_reward[i])\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_reward[i])\n",
    "                print(verbose_str)\n",
    "    \n",
    "        # Update model\n",
    "        total_norms = finish_episode(agents[0:num_learners], optimizers[0:num_learners], gamma, cuda)\n",
    "\n",
    "        if (ep+prior_eps+1) % log_interval == 0:\n",
    "            print('Max Norms = ',[\"%0.2f\" % i for i in total_norms])\n",
    "        \n",
    "        if (ep+prior_eps+1) % save_interval == 0: \n",
    "            for i in range(num_learners):\n",
    "                model_dir = 'MA_models/3T-9L/'\n",
    "                results_dir = 'results/3T-9L/'\n",
    "\n",
    "                model_file = model_dir+'{}/p{}_r{}/MA{}_{}_ep{}.p'.format(culture['name'], culture['penalty'],\\\n",
    "                                 culture['reward'], i, game, ep+prior_eps+1)\n",
    "                data_file = results_dir+'{}/p{}_r{}/MA{}_{}.p'.format(culture['name'], culture['penalty'],\\\n",
    "                                                      culture['reward'], i, game)\n",
    "                \n",
    "                os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "                os.makedirs(os.path.dirname(data_file), exist_ok=True)\n",
    "                \n",
    "                with open(model_file, 'wb') as f:\n",
    "                    # Model Save and Load Update: Include both model and optim parameters \n",
    "                    pickle.dump((agents[i].cpu(), optimizers[i]), f)\n",
    "\n",
    "                if cuda:\n",
    "                    agents[i] = agents[i].cuda()\n",
    "\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    pickle.dump(running_rewards[i], f)    \n",
    "            \n",
    "    env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
