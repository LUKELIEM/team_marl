{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Performance Assessment\n",
    "\n",
    "We implement multi-episode play in order to better assess how good a team or a culture is. The output will be averaged results over these episodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Python version:  3.6.4\n",
      "Pytorch version: 0.4.1.post2\n",
      "OpenAI Gym version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# This is the Gathering Game Environment based on Tribal Organization of agents\n",
    "from tribes_env import GatheringEnv\n",
    "from tribes_model import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"OpenAI Gym version: {}\".format(gym.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load random agent 8\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 427.3\n",
      "Num laser fired = 378.1\n",
      "Total US Hit (friendly fire) = 20.0\n",
      "Total THEM Hit = 131.0\n",
      "friendly fire (%) = 0.133\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 96.0\n",
      "Tribe Saxons has total reward of 219.6\n",
      "Tribe Franks has total reward of 111.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 of Vikings aggressiveness is 0.00\n",
      "Agent0 reward is 36.7\n",
      "US agents hit = 0.0\n",
      "THEM agents hit = 0.0\n",
      "Agent1 of Vikings aggressiveness is 0.00\n",
      "Agent1 reward is 28.2\n",
      "US agents hit = 0.0\n",
      "THEM agents hit = 0.0\n",
      "Agent2 of Vikings aggressiveness is 0.00\n",
      "Agent2 reward is 31.2\n",
      "US agents hit = 0.0\n",
      "THEM agents hit = 0.0\n",
      "Agent3 of Saxons aggressiveness is 0.25\n",
      "Agent3 reward is 169.1\n",
      "US agents hit = 12.1\n",
      "THEM agents hit = 88.6\n",
      "Agent4 of Saxons aggressiveness is 0.13\n",
      "Agent4 reward is 50.5\n",
      "US agents hit = 8.0\n",
      "THEM agents hit = 42.4\n",
      "Agent5 of Franks aggressiveness is 0.00\n",
      "Agent5 reward is 34.4\n",
      "US agents hit = 0.0\n",
      "THEM agents hit = 0.0\n",
      "Agent6 of Franks aggressiveness is 0.00\n",
      "Agent6 reward is 30.0\n",
      "US agents hit = 0.0\n",
      "THEM agents hit = 0.0\n",
      "Agent7 of Franks aggressiveness is 0.00\n",
      "Agent7 reward is 47.4\n",
      "US agents hit = 0.0\n",
      "THEM agents hit = 0.0\n",
      "Training time per epochs: 3.25 sec\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dir_name = 'MA_models/no_fragging/p-1.0/'\n",
    "episodes = 2000  # This is used to recall a model file trained to a # of episodes\n",
    "\n",
    "# There will be 9 agents - 2 teams of 4 AI agents each and 1 random agents\n",
    "num_ai_agents = 8\n",
    "num_rdn_agents = 1\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Initialize environment\n",
    "render = False\n",
    "num_actions = 8                       # There are 8 actions defined in Gathering\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 4\n",
    "max_episodes = 30\n",
    "max_frames = 1000\n",
    "verbose = False\n",
    "\n",
    "def unpack_env_obs(env_obs):\n",
    "    \"\"\"\n",
    "    Gathering is a partially-observable Markov Game. env_obs returned by GatheringEnv is a numpy \n",
    "    array of dimension (num_agent, 800), which represents the agents' observations of the game.\n",
    "\n",
    "    The 800 elements (view_box) encodes 4 layers of 10x20 pixels frames in the format:\n",
    "    (viewbox_width, viewbox_depth, 4).\n",
    "    \n",
    "    This code reshapes the above into stacked frames that can be accepted by the Policy class:\n",
    "    (batch_idx, in_channel, width, height)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    num_agents = len(env_obs)  # environ observations is a list of agents' observations\n",
    "    \n",
    "    obs = []\n",
    "    for i in range(num_agents):\n",
    "        x = env_obs[i]   # take the indexed agent's observation\n",
    "        x = torch.Tensor(x)   # Convert to tensor\n",
    "        \n",
    "        # Policy is a 3-layer CNN\n",
    "        x = x.view(1, 10, 20, -1)  # reshape into environment defined stacked frames\n",
    "        x = x.permute(0, 3, 1, 2)  # permute to Policy accepted stacked frames\n",
    "        obs.append(x)\n",
    "        \n",
    "    return obs  # return a list of Policy accepted stacked frames (tensor)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For now, we do not implement LSTM            \n",
    "# LSTM Change: Need to cycle hx and cx thru function\n",
    "def select_action(model, state, lstm_hc, cuda):\n",
    "    hx , cx = lstm_hc \n",
    "    num_frames, height, width = state.shape\n",
    "    state = torch.FloatTensor(state.reshape(-1, num_frames, height, width))\n",
    "\n",
    "    if cuda:\n",
    "        state = state.cuda()\n",
    "\n",
    "    probs, value, (hx, cx) = model((Variable(state), (hx, cx)))\n",
    "\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    # LSTM Change: Need to cycle hx and cx thru function\n",
    "    return action.data[0], log_prob, value, (hx, cx)\n",
    "\"\"\"\n",
    "\n",
    "def select_action(model, obs, cuda):\n",
    "    \"\"\"\n",
    "    This code expects obs to be an array of stacked frames of the following dim:\n",
    "    (batch_idx, in_channel, width, height)\n",
    "    \n",
    "    This is inputted into model - the agent's Policy, which outputs a probability \n",
    "    distribution over available actions.\n",
    "    \n",
    "    Policy gradient is implemented using torch.distributions.Categorical. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Policy is a 3-layer CNN\n",
    "    # _, num_frames, width, height = obs.shape\n",
    "    # obs = torch.FloatTensor(obs.reshape(-1, num_frames, width, height))\n",
    "    \n",
    "    # Policy is a 2-layer NN for now\n",
    "    # obs = obs.view(1, -1)\n",
    "   \n",
    "    if cuda:\n",
    "        obs = obs.cuda()\n",
    "      \n",
    "    probs = model(obs)\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "\n",
    "    return action.item(), log_prob \n",
    "\n",
    "\n",
    "def load_info(agents, narrate=False):\n",
    "    for i in range(num_agents):    \n",
    "        agents[i].load_info(info[i])\n",
    "        if narrate:\n",
    "            if agents[i].tagged:\n",
    "                print('frame {}, agent{} is tagged'.format(frame,i))\n",
    "            if agents[i].laser_fired:\n",
    "                print('frame {}, agent{} fires its laser'.format(frame,i))\n",
    "                print('and hit {} US and {} THEM'.format(agents[i].US_hit, agents[i].THEM_hit))\n",
    "    return\n",
    "\n",
    "\n",
    "# Load models for AI agents\n",
    "if episodes > 0:\n",
    "    agents= [[] for i in range(num_ai_agents)]\n",
    "    # If episodes is provided (not 0), load the model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        model_file = dir_name+'MA{}_Gather__ep{}.p'.format(i,episodes)\n",
    "        try:\n",
    "            with open(model_file, 'rb') as f:\n",
    "                # Model File include both model and optim parameters\n",
    "                saved_model = pickle.load(f)\n",
    "                agents[i], _ = saved_model\n",
    "                print(\"Load saved model for agent {}\".format(i))\n",
    "        except OSError:\n",
    "            print('Model file not found.')\n",
    "            raise\n",
    "else:\n",
    "    # If episodes=0, start with a freshly initialized model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        print(\"Load AI agent {}\".format(i))\n",
    "        agents.append(Policy(num_frames, num_actions, i))\n",
    "\n",
    "# Load random agents    \n",
    "for i in range(num_ai_agents,num_agents):\n",
    "    print(\"Load random agent {}\".format(i))\n",
    "    agents.append(Rdn_Policy())\n",
    "\n",
    "# Establish tribal association\n",
    "tribes = []\n",
    "tribes.append(Tribe(name='Vikings',color='blue', agents=[agents[0], agents[1], agents[2]]))\n",
    "tribes.append(Tribe(name='Saxons', color='red', agents=[agents[3], agents[4]]))\n",
    "tribes.append(Tribe(name='Franks', color='purple', agents=[agents[5], agents[6], agents[7]]))\n",
    "tribes.append(Tribe(name='Crazies', color='yellow', agents=[agents[8]]))   # random agents are crazy!!!\n",
    "\n",
    "# 9 agents in 4 tribes, used map defined in default.txt\n",
    "agent_colors = [agent.color for agent in agents]\n",
    "agent_tribes = [agent.tribe for agent in agents]\n",
    "env = GatheringEnv(n_agents=num_agents,agent_colors=agent_colors, agent_tribes=agent_tribes, map_name='default')  \n",
    "\n",
    "# Used to accumulate episode stats for averaging\n",
    "cum_rewards = 0\n",
    "cum_tags = 0\n",
    "cum_US_hits = 0\n",
    "cum_THEM_hits = 0\n",
    "cum_agent_rewards = [0 for agent in agents]\n",
    "cum_agent_tags = [0 for agent in agents]\n",
    "cum_agent_US_hits = [0 for agent in agents]\n",
    "cum_agent_THEM_hits = [0 for agent in agents]\n",
    "cum_tribe_rewards = [0 for t in tribes if t.name is not 'Crazies']\n",
    "\n",
    "cuda = False\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(max_episodes):\n",
    "    \n",
    "    print('.', end='')  # To show progress\n",
    "    \n",
    "    # Initialize AI and random agent data\n",
    "    actions = [0 for i in range(num_agents)]\n",
    "    tags = [0 for i in range(num_agents)]\n",
    "    US_hits = [0 for i in range(num_agents)]\n",
    "    THEM_hits = [0 for i in range(num_agents)]\n",
    "\n",
    "    env_obs = env.reset()  # Environment return observations\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack observations into data structure compatible with agent Policy\n",
    "    agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "    for i in range(num_ai_agents):    # Reset agent info - laser tag statistics\n",
    "        agents[i].reset_info()    \n",
    "    \n",
    "    if render:\n",
    "        env.render()\n",
    "        time.sleep(1/15)  # Change speed of video rendering\n",
    "    \n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "    state = np.stack([state]*num_frames)\n",
    "\n",
    "    # Reset LSTM hidden units when episode begins\n",
    "    cx = Variable(torch.zeros(1, 256))\n",
    "    hx = Variable(torch.zeros(1, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    for frame in range(max_frames):\n",
    "\n",
    "        for i in range(num_ai_agents):    # For AI agents\n",
    "            actions[i], _ = select_action(agents[i], agents_obs[i], cuda=cuda)\n",
    "            if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "        for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "            actions[i] = agents[i].select_action(agents_obs[i])\n",
    "            if actions[i] is 6:\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "        \"\"\"\n",
    "        For now, we do not implement LSTM\n",
    "        # Select action\n",
    "        action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "        \"\"\"\n",
    "\n",
    "        # if frame % 10 == 0:\n",
    "        #     print (actions)    \n",
    "            \n",
    "        # Perform step        \n",
    "        env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        For Debug only\n",
    "        print (env_obs)\n",
    "        print (reward)\n",
    "        print (done) \n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(num_ai_agents):\n",
    "            agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        load_info(agents, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            US_hits[i] += agents[i].US_hit\n",
    "            THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "        \"\"\"\n",
    "        For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "        # Evict oldest diff add new diff to state\n",
    "        next_state = np.stack([next_state]*num_frames)\n",
    "        next_state[1:, :, :] = state[:-1, :, :]\n",
    "        state = next_state\n",
    "        \"\"\"\n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(1/15)  # Change speed of video rendering\n",
    "\n",
    "        if any(done):\n",
    "            print(\"Done after {} frames\".format(frame))\n",
    "            break\n",
    "            \n",
    "    # Print out statistics of AI agents\n",
    "    ep_rewards = 0\n",
    "    ep_tags = 0\n",
    "    ep_US_hits = 0\n",
    "    ep_THEM_hits = 0\n",
    "\n",
    "    if verbose:\n",
    "        print ('\\nStatistics by Agent')\n",
    "        print ('===================')\n",
    "    for i in range(num_ai_agents):\n",
    "        agent_tags = sum(agents[i].tag_hist)\n",
    "        ep_tags += agent_tags\n",
    "        cum_agent_tags[i] += agent_tags\n",
    "\n",
    "        agent_reward = sum(agents[i].rewards)\n",
    "        ep_rewards += agent_reward\n",
    "        cum_agent_rewards[i] += agent_reward\n",
    "\n",
    "        agent_US_hits = sum(agents[i].US_hits)\n",
    "        agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "        ep_US_hits += agent_US_hits\n",
    "        ep_THEM_hits += agent_THEM_hits\n",
    "        cum_agent_US_hits[i] += agent_US_hits\n",
    "        cum_agent_THEM_hits[i] += agent_THEM_hits\n",
    "        \n",
    "        if verbose:\n",
    "            print (\"Agent{} aggressiveness is {:.2f}\".format(i, agent_tags/frame))\n",
    "            print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "            print('US agents hit = {}'.format(agent_US_hits))\n",
    "            print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "        \n",
    "    cum_rewards += ep_rewards\n",
    "    cum_tags += ep_tags\n",
    "    cum_US_hits += ep_US_hits\n",
    "    cum_THEM_hits += ep_THEM_hits\n",
    "    \n",
    "    if verbose:\n",
    "        print ('\\nStatistics in Aggregate')\n",
    "        print ('=======================')\n",
    "        print ('Total rewards gathered = {}'.format(ep_rewards))\n",
    "        print ('Num laser fired = {}'.format(ep_tags))\n",
    "        print ('Total US Hit (friendly fire) = {}'.format(ep_US_hits))\n",
    "        print ('Total THEM Hit = {}'.format(ep_THEM_hits))\n",
    "        print ('friendly fire (%) = {0:.3f}'.format(ep_US_hits/(ep_US_hits+ep_THEM_hits+1e-7)))\n",
    "\n",
    "    if verbose:\n",
    "        print ('\\nStatistics by Tribe')\n",
    "        print ('===================')\n",
    "    for i, t in enumerate(tribes):\n",
    "        if t.name is not 'Crazies':\n",
    "            ep_tribe_reward = sum(t.sum_rewards())\n",
    "            cum_tribe_rewards[i] += ep_tribe_reward\n",
    "            if verbose:\n",
    "                print ('Tribe {} has total reward of {}'.format(t.name, ep_tribe_reward))\n",
    "\n",
    "    for i in range(num_ai_agents):\n",
    "        agents[i].clear_history()\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "end = time.time()\n",
    "\n",
    "print ('\\nAverage Statistics in Aggregate')\n",
    "print ('=================================')\n",
    "print ('Total rewards gathered = {:.1f}'.format(cum_rewards/max_episodes))\n",
    "print ('Num laser fired = {:.1f}'.format(cum_tags/max_episodes))\n",
    "print ('Total US Hit (friendly fire) = {:.1f}'.format(cum_US_hits/max_episodes))\n",
    "print ('Total THEM Hit = {:.1f}'.format(cum_THEM_hits/max_episodes))\n",
    "print ('friendly fire (%) = {:.3f}'.format(cum_US_hits/(cum_US_hits+cum_THEM_hits+1e-7)))\n",
    "\n",
    "print ('\\nAverage Statistics by Tribe')\n",
    "print ('=============================')\n",
    "for i, t in enumerate(tribes):\n",
    "    if t.name is not 'Crazies':\n",
    "        print ('Tribe {} has total reward of {:.1f}'.format(t.name, cum_tribe_rewards[i]/max_episodes))    \n",
    "\n",
    "print ('\\nAverage Statistics by Agent')\n",
    "print ('=============================')\n",
    "for i in range(num_ai_agents):\n",
    "    print (\"Agent{} of {} aggressiveness is {:.2f}\".format(i, agents[i].tribe, \\\n",
    "                                                           cum_agent_tags[i]/(max_episodes*max_frames)))\n",
    "    print (\"Agent{} reward is {:.1f}\".format(i, cum_agent_rewards[i]/max_episodes))\n",
    "    print('US agents hit = {:.1f}'.format(cum_agent_US_hits[i]/max_episodes))\n",
    "    print('THEM agents hit = {:.1f}'.format(cum_agent_THEM_hits[i]/max_episodes))\n",
    "\n",
    "print('Training time per epochs: {:.2f} sec'.format((end-start)/max_episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
